---
title: 'R with Big Data 6: Spark Extensions'
author: "John Mount"
output:
  md_document:
    variant: markdown_github
---

This is only a concept script, it runs correctly but is intended for
teaching not direct use in production.

One point in particular is this script assumes none of the following directories are present (as it is going to try to create them and write its own temp results):

  * Exercises/solutions/df*_tmp
  * Exercises/solutions/tmpFile*_*

We don't delete these as we don't want to perform too many (potentially unsafe) file operations on the user's behalf.



```{r setup}
Sys.setenv(TZ='UTC')
library("sparklyr")
library("dplyr")
sc <- spark_connect(master = "local", version = "2.0.0", hadoop_version="2.7")
```


```{r datesexample}
d <- data_frame(x = c(20100101120101, "2009-01-02 12-01-02", "2009.01.03 12:01:03",
       "2009-1-4 12-1-4",
       "2009-1, 5 12:1, 5",
       "200901-08 1201-08",
       "2009 arbitrary 1 non-decimal 6 chars 12 in between 1 !!! 6",
       "OR collapsed formats: 20090107 120107 (as long as prefixed with zeros)",
       "Automatic wday, Thu, detection, 10-01-10 10:01:10 and p format: AM",
       "Created on 10-01-11 at 10:01:11 PM"))

df  <- copy_to(sc, d, 'df')
print(df)
```

Running `SQL` directly (see [http://spark.rstudio.com](http://spark.rstudio.com)).

```{r sql}
library("DBI")

# returns a in-memor data.frame
dfx <- dbGetQuery(sc, "SELECT * FROM df LIMIT 5")
dfx

# build another table
dbSendQuery(sc, "CREATE TABLE df2 AS SELECT * FROM df LIMIT 5")
# get a handle to it
dbListTables(sc)
df2 <- dplyr::tbl(sc, 'df2')
df2
```


Using `SparkR` for `R` user defined functions.

The following doesn't always run in a knitr evironment.  And using `SparkR` in production would entail already having the needed R packages installed.

```{r sparkr}
# Connect via SparkR, more notes: https://github.com/apache/spark/tree/master/R
SPARK_HOME <- sc$spark_home
# https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/KDDCup2016/Code/SparkR/SparkR_sparklyr_NYCTaxi.Rmd
# http://sbartek.github.io/sparkRInstall/installSparkReasyWay.html
library(SparkR, lib.loc = paste0(SPARK_HOME, "/R/lib/"))
sr <- sparkR.session(master = "local", sparkHome = SPARK_HOME)

sparklyr::spark_write_parquet(df, 'df_tmp')
dSparkR <- SparkR::read.df('df_tmp')
# http://spark.apache.org/docs/latest/sparkr.html
schema <- structType(structField("dateStrOrig", "string"), 
                     structField("dateStrNorm", "timestamp"),
                     structField("dateSec", "double"))
dSparkR2 <- SparkR::dapply(dSparkR, function(x) {
  if(!require('lubridate', quietly = TRUE)) {
    install.packages("lubridate", repos= "http://cran.rstudio.com")
  }
  s <- lubridate::ymd_hms(x[[1]])
  x <- cbind(x, s, as.numeric(s))
  x
  }, schema)
SparkR::write.df(dSparkR2, 'dfR_tmp')
dfR <- sparklyr::spark_read_parquet(sc, 'dfR', 'dfR_tmp')
dfR <- dfR %>% 
  dplyr::mutate(dt = from_unixtime(dateSec)) %>%
  dplyr::select(dateStrNorm, dateSec, dt)
print(dfR)
```




From: [http://spark.rstudio.com/extensions.html](http://spark.rstudio.com/extensions.html).

```{r count}
count_lines <- function(sc, file) {
  spark_context(sc) %>% 
    invoke("textFile", file, 1L) %>% 
    invoke("count")
}

count_lines(sc, "tmp.csv")
```

A simple Java example.

```{r trivialexample}
billionBigInteger <- invoke_new(sc, "java.math.BigInteger", "1000000000")
print(billionBigInteger)
str(billionBigInteger)

billion <- invoke(billionBigInteger, "longValue")
str(billion)
```




